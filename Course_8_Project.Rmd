---
title: "Qualitative Analysis of Weight Excercise with Motion Sensors"
author: "Mohit Gupta"
date: "August 21, 2017"
output:html_document variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective
The objective of this project is to build a model that can categorize how well an individual performed a weight lifting excercise.   

# Background
The study was performed by Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. (see references below).  The researchers asked six participants to wear a set of inertial measurement devices and perform several repetitions of unilateral dumbbell bicep curl excercise.  Data streams from different sensors were recorded in moving time windows of various durations.  For each window for each sensor, several aggregate features were calculated and included in the data.

Each repetition of the excercise was classified as a class "A" if it was performed properly and by one of four other classes "B", "C", "D" or "E" if a certain type of movement error was made by the subject.  

Coursera/Johns Hopkins course team took the data generated by the researchers and provided a subset of it for training and testing.  

# A Note on Quality of Data and Futility of this Project
There are several issues with the data provided for this Coursera project -   

1.  Original data had two types of records - a. "instant" data - captured by the sensors at fixed intervals and b. aggregated features - calculated only once per time window.  These two types are mixed in the tabular data.   However, the testing data contains only the "instant" data.  So all the aggregated calculated features in Train are not needde in the model.  

2.  Each row of the "instant" data contains measurements captured at a particular instant.  So, each row, represents only one reading in time while that repetiion was being performed.  Realistically, a good picture of the motions performed by the subject can only be achieved by combining all readings for a repetition in a sequential, chronological order.   The authors have not detailed if or how they achieved this.  

3.  In absence of above, one can only make predictions if the instant data in the test set, somehow matches one instant of the train set - well enough to predict the outcome. This is happily guaranteed, since the test data are a subset of the full set.  

4.  It is very unlikely that this data or models produced here can classify the same excercise done by a new subject.   For that to happen, the model must be built on the typical motion profile time-series of the subjects for one full repetition - i.e. a time-domain combination of all windows.  

5.  So, this appears to be a futile excercise, but we respectfully comply and do what has been asked.  

# Training Data Load and Cleanup  
- Remove initial colums that we don't want to use in the model. user_name is also removed as presumably, we want to build a model that is generalized and can be used for checking the technique of anyone.  

- Remove all summarized rows from Train as the testing data has no summary rows (new_window"=yes). Unfortunately, all the calculated features that existed only in the summary rows will be lost as well and will not be used in the model.  

- Delete all columns where all the values are blank or NA - These columns are all summary features and they are not populated on the "instant" data rows.  

- Convert outcome variable "classe" to factor.  

```{r libraries_no_echo, message=FALSE, echo=FALSE}
library(doParallel) 
library(randomForest)  ## load  explicitly or predict() fails if saved model is reloaded
library(caret)
```

```{r data_load_cleanup}
# Load training data
train <- read.csv("pml-training.csv", stringsAsFactors=FALSE)

# Drop aggregate rows
train <- subset(train, new_window == "no")

# Drop unneeded variables
col_nos_to_delete <- match(c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2",
				  	          "cvtd_timestamp","new_window","num_window"
							 ),	colnames(train))
train <- train[, -col_nos_to_delete]

# Drop all calculated aggregate features - they are blank or NA on instant data
cols_to_keep <-sapply(train, function(c) ifelse(all(c=="" || is.na(c)), FALSE, TRUE))
train <- train[, cols_to_keep]
train$classe <- factor(train$classe)
```

After this, we are left with `r nrow(train)` rows with `r ncol(train)` columns (including the outcome).  

None of the variables had near-zero variance to qualify for dropping.  
```{r check_nz_variance}
nz<-nearZeroVar(train, saveMetrics= TRUE)
sum(nz$nzv==TRUE)
```

Checked if some variables have very high correlation and can be dropped.  Following nine predictors have more than 0.9 correlation to one or more predictors.  However, I decided not to remove them as I am going to do random forests and 7 out of 52 correlated variables should not affect the outcome too much.  Since the RF takes so long, I was not able to check if removing these improves or deteriorates the results.  

```{r check_covariance} 
findCorrelation(cor(train[,1:52]))
```

# Try 1 - Full Random Forest with Default Values  

First I ran a full random forest with everything set to default (except I used parallel processing).  It ran for just under an hour but gave extremely good results and OOB error prediction.  saved the model so I can just load it back instead of rerunning.
```{r rf1, echo=TRUE, message=TRUE}
# Full Random Forest
if (file.exists("rf1FitSaved.RData")) { 
	load("rf1FitSaved.RData")
} else {
	set.seed(100)
	cluster <- makeCluster(detectCores()) # use all CPU threads and nap for an hour
	registerDoParallel(cluster)

	Sys.time()
	rf1Fit <- train(x=train[,1:52], y=train$classe, method="rf", allowParallel = TRUE)
	Sys.time()
	
	stopCluster(cluster)
	registerDoSEQ()
	
	save(rf1Fit, file="rf1FitSaved.RData")
}
rf1Fit$times$everything
confusionMatrix(predict(rf1Fit$finalModel), train$classe)
plot(rf1Fit$finalModel)
```

### The full RF model took `r rf1Fit$times$everything[3]/60` minutes to run.  
### Accuracy was 99.6% (with 95% CI of 99.5 to 99.68).  
### The grapahic shows that there is not much benefit to overall OOB error rate after about 200 trees have been built.  

# Try 2 - Quicker Random Forest

To see if I can build an accurate model faster, I cut ntree to 200 and instead of using bootstrapping, use repeated cross-validation (5 folds, repeated 10 times).  

```{r rf2, echo=TRUE, message=TRUE}
# Quicker l Random Forest
if (file.exists("rfFit2Saved.RData")) { 
	load("rfFit2Saved.RData")
} else {
	set.seed(100)
	cluster <- makeCluster(detectCores()) # use all CPU threads and get coffee
	registerDoParallel(cluster)
	
	Sys.time()
	rf2TrControl <- trainControl(method="repeatedcv", number=5, repeats=10,  allowParallel=TRUE)
	rfFit2 <- train(x=train[,1:52], y=train$classe, method="rf", ntree=200, trControl=rf2TrControl)
	Sys.time()
	
	stopCluster(cluster)
	registerDoSEQ()
	
	save(rfFit2, file="rf2FitSaved.RData")
}
rfFit2$times$everything
confusionMatrix(predict(rfFit2$finalModel), train$classe)
```

### This RF model took only `r rfFit2$times$everything[3]/60` minutes to run.  
### Accuracy was 99.53% (with 95% CI of 99.42 to 99.62).  Accuracy came down negligibly.
### Overall OOB error rate with 200 trees was `r tail(rfFit2$finalModel$err.rate,1 )[1]`.  

# Try 3 - Lightning Fast Random Forest With PCA

Time taken to generate a random forest model, increases as the square of number of predictors.  I decided to run PCA on the predictors to choose only those components that contribute to top 95% of variance.  Then I ran RF on the reduced number of predictors. ntree of 250 and OOB cross-validation instead of k-fold..   

```{r rf3, echo=TRUE, message=TRUE}
# Lightning Fast Random Forest by doing PCA first to reduce number of predictors

PP <- preProcess(x=train[,1:52], method=c("center","scale","pca"), verbose=FALSE, thresh=0.95)
trainPCA <- predict(PP, newdata=train[,1:52])

if (file.exists("rfFit3Saved.RData")) { 
	load("rfFit3Saved.RData")
} else {
	set.seed(100)
	cluster <- makeCluster(detectCores()) # use all CPU threads
	registerDoParallel(cluster)

	Sys.time()
	rf3TrControl <- trainControl(method="oob", number=5, allowParallel=TRUE)
	rfFit3 <- train(trainPCA, y=train$classe, method="rf", ntree=250, trControl=rf3TrControl)
	Sys.time()
	
	stopCluster(cluster)
	registerDoSEQ()
	
	save(rfFit3, file="rf2FitSaved.RData")
}
rfFit3$times$everything
confusionMatrix(predict(rfFit3$finalModel), train$classe)
```

### PCA found that only `r PP$numComp` components add up to the top 95% variance. RF should be much faster with fewer predictors.
### This RF model took only `r rfFit3$times$everything[3]/60` minutes to run!!!! 
### Accuracy was 98.21% (with 95% CI of 98.01 to 98.39).  Accuracy came down slightly, but still very good.
### Overall OOB error rate with 250 trees was `r tail(rfFit3$finalModel$err.rate,1 )[1]`.

# Final Prediction  
Since I have already built 3 RF models, I decided to predict the test dataset with all 3.  I also tried a couple of other models but none works as well as RF.  Of course RF takes a lot of time to run, but it can be significantly accelerated by reducing the number of predictors when possible and not doing very heavy cross validation.   

```{r prediction}
# Load Test Data
test  <- read.csv("pml-testing.csv",  stringsAsFactors=FALSE)

P1 <- predict(rf1Fit, newdata=test)
P2 <- predict(rfFit2, newdata=test)

testPC <- predict(PP, newdata=test[,colnames(train)[1:52]])  ## Run same PCA on Test using column names from Train
P3 <- predict(rfFit3, newdata=testPC)

P1; P2; P3
all(P1==P2)
all(P1==P3)
```
### All three models result in the exact same predictions on Test  
### Model 3 completed super-fast so I would use this technique in future   
### RF gave the best results out of all methods I tried for this particular problem.   



# References  
1. Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.   

2. Project Description : http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har#ixzz4q96OCVds  downloaded on August 21, 2017 at 9:23 PM Central US Time.  

3.  Training data provided: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  

4.  Test data provided: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  

